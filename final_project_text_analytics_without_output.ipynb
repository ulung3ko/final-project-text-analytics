{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Installing Required Dependencies\n",
        "\n",
        "This cell installs all external libraries required for the project to run smoothly in the Google Colab environment. These include packages for transformer-based models, dataset processing, metrics evaluation, and visualization. This step ensures that all necessary tools are available before executing the main pipeline."
      ],
      "metadata": {
        "id": "ZAvVbtdsJTVr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYbYhtRGVFGR"
      },
      "outputs": [],
      "source": [
        "# Install required dependencies\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def install_package(package):\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "\n",
        "packages = [\n",
        "    \"openpyxl\",\n",
        "    \"langdetect\",\n",
        "    \"pandas\",\n",
        "    \"numpy\",\n",
        "    \"transformers\",\n",
        "    \"datasets\",\n",
        "    \"evaluate\",\n",
        "    \"torch\",\n",
        "    \"matplotlib\",\n",
        "    \"seaborn\",\n",
        "    \"accelerate\",\n",
        "    \"plotly\"\n",
        "]\n",
        "\n",
        "for package in packages:\n",
        "    try:\n",
        "        if package == \"transformers\":\n",
        "            import transformers\n",
        "        elif package == \"datasets\":\n",
        "            import datasets\n",
        "        elif package == \"evaluate\":\n",
        "            import evaluate\n",
        "        elif package == \"torch\":\n",
        "            import torch\n",
        "        elif package == \"accelerate\":\n",
        "            import accelerate\n",
        "        elif package == \"plotly\":\n",
        "            import plotly\n",
        "        else:\n",
        "            __import__(package.replace(\"-\", \"_\"))\n",
        "    except ImportError:\n",
        "        print(f\"Installing {package}...\")\n",
        "        install_package(package)\n",
        "\n",
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import glob\n",
        "import time\n",
        "import gc\n",
        "from itertools import product\n",
        "from langdetect import detect, DetectorFactory\n",
        "from google.colab import drive\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorWithPadding,\n",
        "    EarlyStoppingCallback\n",
        ")\n",
        "from datasets import Dataset\n",
        "import evaluate\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set seed for reproducibility\n",
        "DetectorFactory.seed = 0\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Check for GPU and setup\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "\n",
        "# Memory management for Colab\n",
        "def cleanup_memory():\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mounting Google Drive\n",
        "\n",
        "This cell mounts the user's Google Drive to the current Colab session. All datasets, model checkpoints, and result files are stored and accessed through Google Drive to maintain persistence and avoid data loss between sessions. This step is essential for managing larger files and organizing experimental outputs."
      ],
      "metadata": {
        "id": "QuE7XiXGJzx-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "print(\"Mounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Read and Combine Data\n",
        "print(\"Loading dataset files...\")\n",
        "data_path = '/content/drive/MyDrive/Thesis/dataset/Indonesia/'\n",
        "xlsx_files = glob.glob(os.path.join(data_path, '*.xlsx'))\n",
        "\n",
        "if not xlsx_files:\n",
        "    print(f\"No .xlsx files found in {data_path}\")\n",
        "    print(\"Please check the path and ensure files exist.\")\n",
        "else:\n",
        "    print(f\"Found {len(xlsx_files)} .xlsx files\")\n",
        "\n",
        "all_reviews = []\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Clean text by removing HTML, emojis, non-ASCII, and extra spaces\"\"\"\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "\n",
        "    text = str(text)\n",
        "    text = re.sub(r'<[^>]+>', '', text)\n",
        "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = text.strip().lower()\n",
        "\n",
        "    return text\n",
        "\n",
        "def is_english(text):\n",
        "    \"\"\"Check if text is predominantly in English\"\"\"\n",
        "    try:\n",
        "        if len(text.strip()) < 10:\n",
        "            return False\n",
        "        return detect(text) == 'en'\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "for file_path in xlsx_files:\n",
        "    try:\n",
        "        print(f\"Processing {os.path.basename(file_path)}...\")\n",
        "        df = pd.read_excel(file_path)\n",
        "\n",
        "        if 'Review Title' not in df.columns or 'Review Text' not in df.columns or 'Rating' not in df.columns:\n",
        "            print(f\"Skipping {file_path}: Missing required columns\")\n",
        "            continue\n",
        "\n",
        "        df['text'] = df['Review Title'].astype(str) + ' ' + df['Review Text'].astype(str)\n",
        "        df = df.dropna(subset=['text', 'Rating'])\n",
        "        df = df[df['text'].str.len() > 10]\n",
        "        df['text'] = df['text'].apply(clean_text)\n",
        "        df = df[df['text'].str.len() > 5]\n",
        "\n",
        "        print(f\"Filtering for English text...\")\n",
        "        df['is_english'] = df['text'].apply(is_english)\n",
        "        df = df[df['is_english'] == True]\n",
        "        df = df.drop('is_english', axis=1)\n",
        "        df = df[['text', 'Rating']].copy()\n",
        "\n",
        "        all_reviews.append(df)\n",
        "        print(f\"Added {len(df)} English reviews from {os.path.basename(file_path)}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {file_path}: {e}\")\n",
        "\n",
        "if not all_reviews:\n",
        "    print(\"No valid data found. Please check your files.\")\n",
        "else:\n",
        "    combined_df = pd.concat(all_reviews, ignore_index=True)\n",
        "    print(f\"\\nTotal combined reviews: {len(combined_df)}\")\n",
        "\n",
        "# Aspect Labeling\n",
        "print(\"\\nApplying aspect labeling...\")\n",
        "\n",
        "digital_keywords = [\n",
        "    'digital', 'wifi', 'internet', 'charging', 'application', 'self check-in',\n",
        "    'self service', 'qr code', 'technology', 'website', 'touchscreen',\n",
        "    'cashless', 'online booking', 'digital map', 'barcode', 'scan',\n",
        "    'mobile app', 'online system', 'connectivity', 'device'\n",
        "]\n",
        "\n",
        "service_keywords = [\n",
        "    'service', 'staff', 'employee', 'security', 'cashier', 'greeting',\n",
        "    'customer service', 'hospitality', 'slow', 'helpful', 'unhelpful',\n",
        "    'rude', 'polite', 'friendly', 'unfriendly', 'queue', 'waiting',\n",
        "    'assistance', 'responsiveness', 'behavior'\n",
        "]\n",
        "\n",
        "def assign_aspect(text):\n",
        "    text_lower = text.lower()\n",
        "    for keyword in digital_keywords:\n",
        "        if keyword in text_lower:\n",
        "            return 'digital_accessibility'\n",
        "    for keyword in service_keywords:\n",
        "        if keyword in text_lower:\n",
        "            return 'customer_service'\n",
        "    return 'other'\n",
        "\n",
        "combined_df['aspect'] = combined_df['text'].apply(assign_aspect)\n",
        "\n",
        "# Sentiment Labeling based on Rating\n",
        "print(\"Applying sentiment labeling based on ratings...\")\n",
        "\n",
        "def assign_sentiment(rating):\n",
        "    if rating >= 4:\n",
        "        return 'positive'\n",
        "    elif rating <= 2:\n",
        "        return 'negative'\n",
        "    else:\n",
        "        return 'neutral'\n",
        "\n",
        "combined_df['sentiment'] = combined_df['Rating'].apply(assign_sentiment)\n",
        "\n",
        "# Data Filtering\n",
        "print(\"\\nFiltering data...\")\n",
        "print(f\"Before filtering: {len(combined_df)} samples\")\n",
        "\n",
        "combined_df = combined_df[combined_df['aspect'] != 'other']\n",
        "print(f\"After removing 'other' aspect: {len(combined_df)} samples\")\n",
        "\n",
        "combined_df = combined_df[combined_df['sentiment'] != 'neutral']\n",
        "print(f\"After removing 'neutral' sentiment: {len(combined_df)} samples\")\n",
        "\n",
        "print(\"\\nFinal class distribution:\")\n",
        "print(\"Aspect distribution:\")\n",
        "print(combined_df['aspect'].value_counts())\n",
        "print(\"\\nSentiment distribution:\")\n",
        "print(combined_df['sentiment'].value_counts())\n",
        "\n",
        "# Dataset Split and Label Encoding\n",
        "print(\"\\nSplitting dataset and encoding labels...\")\n",
        "\n",
        "aspect_labels = sorted(combined_df['aspect'].unique())\n",
        "sentiment_labels = sorted(combined_df['sentiment'].unique())\n",
        "\n",
        "aspect_label2id = {label: i for i, label in enumerate(aspect_labels)}\n",
        "aspect_id2label = {i: label for label, i in aspect_label2id.items()}\n",
        "\n",
        "sentiment_label2id = {label: i for i, label in enumerate(sentiment_labels)}\n",
        "sentiment_id2label = {i: label for label, i in sentiment_label2id.items()}\n",
        "\n",
        "print(f\"Aspect labels: {aspect_labels}\")\n",
        "print(f\"Sentiment labels: {sentiment_labels}\")\n",
        "\n",
        "combined_df['aspect_labels'] = combined_df['aspect'].map(aspect_label2id)\n",
        "combined_df['sentiment_labels'] = combined_df['sentiment'].map(sentiment_label2id)\n",
        "\n",
        "# Split data\n",
        "X = combined_df['text']\n",
        "y_aspect = combined_df['aspect_labels']\n",
        "y_sentiment = combined_df['sentiment_labels']\n",
        "\n",
        "stratify_key = combined_df['aspect'].astype(str) + '_' + combined_df['sentiment'].astype(str)\n",
        "\n",
        "X_temp, X_test, y_aspect_temp, y_aspect_test, y_sentiment_temp, y_sentiment_test = train_test_split(\n",
        "    X, y_aspect, y_sentiment, test_size=0.15, random_state=42, stratify=stratify_key\n",
        ")\n",
        "\n",
        "# Create temp stratify key\n",
        "temp_df = pd.DataFrame({\n",
        "    'text': X_temp,\n",
        "    'aspect_labels': y_aspect_temp,\n",
        "    'sentiment_labels': y_sentiment_temp\n",
        "})\n",
        "temp_df['aspect'] = temp_df['aspect_labels'].map(aspect_id2label)\n",
        "temp_df['sentiment'] = temp_df['sentiment_labels'].map(sentiment_id2label)\n",
        "stratify_temp = temp_df['aspect'].astype(str) + '_' + temp_df['sentiment'].astype(str)\n",
        "\n",
        "X_train, X_val, y_aspect_train, y_aspect_val, y_sentiment_train, y_sentiment_val = train_test_split(\n",
        "    X_temp, y_aspect_temp, y_sentiment_temp, test_size=0.176, random_state=42, stratify=stratify_temp\n",
        ")\n",
        "\n",
        "print(f\"Train set: {len(X_train)} samples\")\n",
        "print(f\"Validation set: {len(X_val)} samples\")\n",
        "print(f\"Test set: {len(X_test)} samples\")"
      ],
      "metadata": {
        "id": "fNTlSg1GiZ4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model and Hyperparameter Configuration\n",
        "\n",
        "This section defines the core configurations for the fine-tuning process. It includes model names, hyperparameters such as learning rate and number of epochs, and other training arguments. Setting these values upfront makes the training process reproducible and easier to manage, especially when experimenting with multiple models."
      ],
      "metadata": {
        "id": "nuitCArAJ5gl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model and Hyperparameter Configuration\n",
        "print(\"\\nSetting up models and hyperparameters...\")\n",
        "\n",
        "# Models to compare\n",
        "models_config = {\n",
        "    'distilbert-base-uncased': {\n",
        "        'name': 'DistilBERT',\n",
        "        'model_name': 'distilbert-base-uncased'\n",
        "    },\n",
        "    'bert-base-uncased': {\n",
        "        'name': 'BERT',\n",
        "        'model_name': 'bert-base-uncased'\n",
        "    },\n",
        "    'roberta-base': {\n",
        "        'name': 'RoBERTa',\n",
        "        'model_name': 'roberta-base'\n",
        "    }\n",
        "}\n",
        "\n",
        "# Hyperparameters to tune (optimized for free Colab T4 GPU)\n",
        "hyperparams = {\n",
        "    'learning_rate': [2e-5, 5e-5],  # Reduced from 3 to 2 options\n",
        "    'per_device_train_batch_size': [8, 16],  # Keep 2 options\n",
        "    'num_train_epochs': [2, 3]  # Keep 2 options but reduced max\n",
        "}\n",
        "\n",
        "# Generate all combinations\n",
        "hyperparam_combinations = list(product(\n",
        "    hyperparams['learning_rate'],\n",
        "    hyperparams['per_device_train_batch_size'],\n",
        "    hyperparams['num_train_epochs']\n",
        "))\n",
        "\n",
        "print(f\"Total hyperparameter combinations per model: {len(hyperparam_combinations)}\")\n",
        "print(f\"Total experiments (ASPECT ONLY): {len(models_config) * len(hyperparam_combinations)}\")  # Only aspect classification\n",
        "\n",
        "# Training and Evaluation Functions\n",
        "def create_datasets(X_train, X_val, X_test, y_train, y_val, y_test, tokenizer, max_length=256):\n",
        "    \"\"\"Create tokenized datasets\"\"\"\n",
        "    def tokenize_function(examples):\n",
        "        return tokenizer(examples['text'], truncation=True, padding=True, max_length=max_length)\n",
        "\n",
        "    train_dataset = Dataset.from_dict({'text': X_train.tolist(), 'labels': y_train.tolist()})\n",
        "    val_dataset = Dataset.from_dict({'text': X_val.tolist(), 'labels': y_val.tolist()})\n",
        "    test_dataset = Dataset.from_dict({'text': X_test.tolist(), 'labels': y_test.tolist()})\n",
        "\n",
        "    train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "    val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
        "    test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "    return train_dataset, val_dataset, test_dataset"
      ],
      "metadata": {
        "id": "S_jo3O8DinyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading Evaluation Metrics\n",
        "\n",
        "This cell imports the evaluation functions used to assess the performance of each model. These typically include precision, recall, F1-score, and accuracy. Defining these metrics explicitly allows for consistent evaluation across all models and hyperparameter settings.\n"
      ],
      "metadata": {
        "id": "kYlS7XFOJ-VQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load evaluation metrics\n",
        "accuracy_metric = evaluate.load(\"accuracy\")\n",
        "f1_metric = evaluate.load(\"f1\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "\n",
        "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)\n",
        "    f1 = f1_metric.compute(predictions=predictions, references=labels, average='weighted')\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy['accuracy'],\n",
        "        'f1': f1['f1']\n",
        "    }\n",
        "\n",
        "def train_model(model_name, train_dataset, val_dataset, test_dataset, num_labels,\n",
        "                id2label, label2id, task_name, lr, batch_size, epochs):\n",
        "    \"\"\"Train a single model with given hyperparameters\"\"\"\n",
        "\n",
        "    print(f\"Training {model_name} - {task_name} | LR: {lr}, BS: {batch_size}, Epochs: {epochs}\")\n",
        "\n",
        "    # Initialize model and tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_name,\n",
        "        num_labels=num_labels,\n",
        "        id2label=id2label,\n",
        "        label2id=label2id\n",
        "    )\n",
        "\n",
        "    # Training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f'./results_{task_name}_{model_name.replace(\"/\", \"_\")}',\n",
        "        num_train_epochs=epochs,\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        per_device_eval_batch_size=batch_size,\n",
        "        learning_rate=lr,\n",
        "        warmup_steps=100,\n",
        "        weight_decay=0.01,\n",
        "        logging_steps=50,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"f1\",\n",
        "        greater_is_better=True,\n",
        "        save_total_limit=1,\n",
        "        report_to=[],\n",
        "        dataloader_pin_memory=False,\n",
        "        gradient_accumulation_steps=2 if batch_size == 8 else 1  # Help with small batch sizes\n",
        "    )\n",
        "\n",
        "    # Data collator\n",
        "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "    # Initialize trainer with early stopping\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator,\n",
        "        compute_metrics=compute_metrics,\n",
        "        callbacks=[EarlyStoppingCallback(early_stopping_patience=1)]\n",
        "    )\n",
        "\n",
        "    # Train model\n",
        "    start_time = time.time()\n",
        "    trainer.train()\n",
        "    training_time = time.time() - start_time\n",
        "\n",
        "    # Evaluate on test set\n",
        "    test_results = trainer.evaluate(test_dataset)\n",
        "\n",
        "    # Get predictions\n",
        "    predictions = trainer.predict(test_dataset)\n",
        "    y_pred = np.argmax(predictions.predictions, axis=1)\n",
        "    y_true = predictions.label_ids\n",
        "\n",
        "    # Cleanup\n",
        "    del model, trainer\n",
        "    cleanup_memory()\n",
        "\n",
        "    return {\n",
        "        'model_name': model_name,\n",
        "        'task': task_name,\n",
        "        'learning_rate': lr,\n",
        "        'batch_size': batch_size,\n",
        "        'epochs': epochs,\n",
        "        'test_accuracy': test_results['eval_accuracy'],\n",
        "        'test_f1': test_results['eval_f1'],\n",
        "        'training_time': training_time,\n",
        "        'predictions': y_pred,\n",
        "        'true_labels': y_true\n",
        "    }"
      ],
      "metadata": {
        "id": "ZmQLg-spiwDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Running Hyperparameter Tuning for All Models\n",
        "\n",
        "This section executes the training and hyperparameter tuning for all candidate models. Each model is fine-tuned on the training set using the configurations defined earlier, and performance is evaluated to select the best-performing setup. This step is computationally intensive and may take a significant amount of time depending on the number of models and hyperparameters tested.\n"
      ],
      "metadata": {
        "id": "BXZ7ndagKB8H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run Hyperparameter Tuning for All Models\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STARTING HYPERPARAMETER TUNING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "results = []\n",
        "\n",
        "for model_key, model_info in models_config.items():\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"TRAINING {model_info['name'].upper()} MODELS\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    model_name = model_info['model_name']\n",
        "\n",
        "    # Initialize tokenizer for this model\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    # Create datasets for aspect classification only\n",
        "    aspect_train_ds, aspect_val_ds, aspect_test_ds = create_datasets(\n",
        "        X_train, X_val, X_test, y_aspect_train, y_aspect_val, y_aspect_test, tokenizer\n",
        "    )\n",
        "\n",
        "    # Train with each hyperparameter combination (ASPECT ONLY)\n",
        "    for lr, batch_size, epochs in hyperparam_combinations:\n",
        "        try:\n",
        "            # Train aspect model only\n",
        "            aspect_result = train_model(\n",
        "                model_name, aspect_train_ds, aspect_val_ds, aspect_test_ds,\n",
        "                len(aspect_labels), aspect_id2label, aspect_label2id,\n",
        "                'aspect', lr, batch_size, epochs\n",
        "            )\n",
        "            results.append(aspect_result)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error training {model_name} with LR={lr}, BS={batch_size}, Epochs={epochs}: {e}\")\n",
        "            cleanup_memory()\n",
        "            continue"
      ],
      "metadata": {
        "id": "PUPY4XiOi3hM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Result Analysis and Visualization\n",
        "\n",
        "After training is complete, this cell analyzes the evaluation results for each model. It may include visual comparisons such as bar plots or line charts for F1-scores, confusion matrices, or training loss curves. The goal is to interpret which model performs best under the defined criteria and justify its selection.\n"
      ],
      "metadata": {
        "id": "T3t22BYnKDt8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Results Analysis and Visualization\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ANALYZING RESULTS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Convert results to DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "print(f\"Total completed experiments: {len(results_df)}\")\n",
        "\n",
        "# Best results for each model-task combination\n",
        "print(\"\\nBest Results by Model and Task:\")\n",
        "best_results = results_df.loc[results_df.groupby(['model_name', 'task'])['test_f1'].idxmax()]\n",
        "\n",
        "for _, row in best_results.iterrows():\n",
        "    model_display = models_config.get(row['model_name'], {}).get('name', row['model_name'])\n",
        "    print(f\"{model_display} - {row['task'].title()}: F1={row['test_f1']:.4f}, \"\n",
        "          f\"Acc={row['test_accuracy']:.4f} | LR={row['learning_rate']}, BS={row['batch_size']}, E={row['epochs']}\")\n",
        "\n",
        "# Comprehensive Visualizations\n",
        "print(\"\\nCreating visualizations...\")\n",
        "\n",
        "# Performance comparison plots - ASPECT ONLY\n",
        "fig = make_subplots(\n",
        "    rows=1, cols=2,\n",
        "    subplot_titles=['Aspect Classification - Accuracy', 'Aspect Classification - F1 Score'],\n",
        "    horizontal_spacing=0.15\n",
        ")\n",
        "\n",
        "# Only aspect results\n",
        "aspect_results = results_df[results_df['task'] == 'aspect']\n",
        "\n",
        "# Group by model for box plots\n",
        "for metric_idx, metric in enumerate(['test_accuracy', 'test_f1']):\n",
        "    col = metric_idx + 1\n",
        "\n",
        "    for model_name in aspect_results['model_name'].unique():\n",
        "        model_data = aspect_results[aspect_results['model_name'] == model_name]\n",
        "        model_display = models_config.get(model_name, {}).get('name', model_name)\n",
        "\n",
        "        fig.add_trace(\n",
        "            go.Box(y=model_data[metric], name=model_display,\n",
        "                  boxpoints='all', jitter=0.3, pointpos=-1.8),\n",
        "            row=1, col=col\n",
        "        )\n",
        "\n",
        "fig.update_layout(height=400, title_text=\"Aspect Classification: Model Performance Comparison\")\n",
        "fig.show()\n",
        "\n",
        "# Best hyperparameters heatmap\n",
        "print(\"\\nCreating hyperparameter analysis...\")\n",
        "\n",
        "# Performance by hyperparameters\n",
        "hyperparam_analysis = results_df.groupby(['task', 'learning_rate', 'batch_size', 'epochs']).agg({\n",
        "    'test_f1': 'mean',\n",
        "    'test_accuracy': 'mean',\n",
        "    'training_time': 'mean'\n",
        "}).reset_index()\n",
        "\n",
        "# Create heatmap for aspect classification only\n",
        "task_data = hyperparam_analysis[hyperparam_analysis['task'] == 'aspect']\n",
        "\n",
        "# Pivot for heatmap\n",
        "heatmap_data = task_data.pivot_table(\n",
        "    values='test_f1',\n",
        "    index=['learning_rate', 'epochs'],\n",
        "    columns='batch_size',\n",
        "    aggfunc='mean'\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(heatmap_data, annot=True, fmt='.3f', cmap='RdYlGn')\n",
        "plt.title('Aspect Classification - F1 Score by Hyperparameters')\n",
        "plt.xlabel('Batch Size')\n",
        "plt.ylabel('Learning Rate, Epochs')\n",
        "plt.show()\n",
        "\n",
        "# Detailed Results Table\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FINAL RESULTS SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Create comprehensive results table\n",
        "summary_table = []\n",
        "for _, row in best_results.iterrows():\n",
        "    model_display = models_config.get(row['model_name'], {}).get('name', row['model_name'])\n",
        "\n",
        "    # Get confusion matrix for best model\n",
        "    if row['task'] == 'aspect':\n",
        "        target_names = [aspect_id2label[i] for i in range(len(aspect_labels))]\n",
        "    else:\n",
        "        target_names = [sentiment_id2label[i] for i in range(len(sentiment_labels))]\n",
        "\n",
        "    summary_table.append({\n",
        "        'Model': model_display,\n",
        "        'Task': row['task'].title(),\n",
        "        'Accuracy': f\"{row['test_accuracy']:.4f}\",\n",
        "        'F1-Score': f\"{row['test_f1']:.4f}\",\n",
        "        'Learning Rate': row['learning_rate'],\n",
        "        'Batch Size': int(row['batch_size']),\n",
        "        'Epochs': int(row['epochs']),\n",
        "        'Training Time (s)': f\"{row['training_time']:.1f}\"\n",
        "    })\n",
        "\n",
        "summary_df = pd.DataFrame(summary_table)\n",
        "print(summary_df.to_string(index=False))"
      ],
      "metadata": {
        "id": "nsaBIhjOi-K7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Confusion Matrices for Best Models (Aspect Prediction Only)\n",
        "\n",
        "This final analysis cell focuses on generating and displaying confusion matrices for the best-performing models, specifically on the aspect classification task. Confusion matrices provide a granular view of model predictions, highlighting which aspect classes are frequently confused and where the model excels or struggles.\n"
      ],
      "metadata": {
        "id": "yQscDKdKN_2K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion Matrices for Best Models (Aspect Only)\n",
        "print(\"\\nConfusion Matrices for Best Aspect Models:\")\n",
        "\n",
        "for _, row in best_results.iterrows():\n",
        "    model_display = models_config.get(row['model_name'], {}).get('name', row['model_name'])\n",
        "    target_names = [aspect_id2label[i] for i in range(len(aspect_labels))]\n",
        "\n",
        "    cm = confusion_matrix(row['true_labels'], row['predictions'])\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=target_names, yticklabels=target_names)\n",
        "    plt.title(f'{model_display} - Aspect Classification (Best Model)')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.show()\n",
        "\n",
        "    # Print classification report\n",
        "    print(f\"\\n{model_display} - Aspect Classification Report:\")\n",
        "    print(classification_report(row['true_labels'], row['predictions'],\n",
        "                              target_names=target_names))\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ASPECT CLASSIFICATION HYPERPARAMETER TUNING COMPLETED!\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Final cleanup\n",
        "cleanup_memory()"
      ],
      "metadata": {
        "id": "r73pf81_jAmg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}